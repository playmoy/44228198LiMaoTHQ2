In [1]:
##Q3(a) 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset=pd.read_csv('/Users/daisydu/Downloads/student_results.csv')

##use read_csv to read the .csv file and name it "dataset".
In [2]:
## b
dataset.fillna(0,inplace=True)
dataset.head()

##use fillna() to replace all the NAN values with 0.
Out[2]:
ID number	Assessment 1	Assessment 2	Online quiz 1	Online quiz 2	Online quiz 3	Online quiz 4	Group assignment 1	Group assignment 2	Assessment 3	Assessment 4	Group assignment 3	Group evaluation	Final exam	Class No	groupname	Student category
0	54002768	17.21	24.19	0.0	0.00	15.00	18.92	26.57	0.00	0.00	0.00	50.0	2.00	69	9	cl9tg2	domestic
1	54005658	66.50	0.00	22.0	18.14	29.00	19.91	22.35	17.04	60.54	24.07	42.5	1.88	63	2	cl2tg8	domestic
2	54010438	0.00	0.00	0.0	0.00	4.13	0.00	18.86	0.00	43.75	0.00	22.5	0.00	15	3	cl3tg7	international
3	54011658	60.67	29.07	27.5	18.40	24.50	18.52	25.00	19.29	66.11	27.61	35.0	1.72	75	20	cl20tg2	domestic
4	54012789	37.33	29.01	11.0	20.00	19.50	19.10	27.75	17.82	55.61	19.21	42.5	2.00	33	19	cl19tg6	domestic
In [4]:
##c.
weights=[[0.07/70],[0.03/30],[0.03/30],[0.02/20],[0.03/30],[0.02/20],[0.024/30],[0.016/20],[0.07/70],[0.03/30],[0.04/50],[0.02/2],[0.6/100]]
dataset['Total.marks']=np.dot(dataset.iloc[:,1:14], weights)*100

##Set the weights for scores in each exam according to table 1
##Use np.dot to calculate the weighted average points for each student and name it "Total.marks".
In [5]:
##c.continue
import seaborn as sb
boxplot=sb.boxplot(data=dataset,y='Total.marks',x='Student category')

## use seaborn.boxplot to plot the variable 'Total.marks' for the two student categories.

In [6]:
##d.
stat_table = dataset.groupby('Class No').agg({"Total.marks": ['mean', 'var', 'std']})
stat_table

##Group the variable 'Total.marks' according to the variable 'Class No' and calculate the mean, variance 
##and standeviation and print it out.
Out[6]:
Total.marks
mean	var	std
Class No			
1	59.280813	242.710561	15.579171
2	60.671069	160.151821	12.655110
3	57.610713	273.495919	16.537712
4	56.220069	319.034109	17.861526
5	57.918085	383.676678	19.587666
6	62.332600	252.025294	15.875305
7	54.879626	344.554792	18.562187
8	54.384800	358.580091	18.936211
9	53.918419	205.356479	14.330264
10	59.459821	286.748324	16.933645
11	53.123421	333.302820	18.256583
12	57.040716	239.145724	15.464337
15	57.521067	221.097589	14.869351
16	61.211431	179.019517	13.379818
17	56.857269	238.727958	15.450824
18	59.823636	242.163125	15.561591
19	56.263841	95.438828	9.769280
20	55.022638	210.342787	14.503199
21	55.151473	247.260917	15.724532
In [8]:
####e.
Grade = []

for s in dataset['Total.marks']:
    if s < 50:Grade.append('F')
    elif s < 65:Grade.append('P')  
    elif s < 75:Grade.append('Cr')
    elif s < 85:Grade.append('D') 
    else:  Grade.append('HD')
      
dataset['Grade']=Grade        
dataset.head()
Out[8]:
ID number	Assessment 1	Assessment 2	Online quiz 1	Online quiz 2	Online quiz 3	Online quiz 4	Group assignment 1	Group assignment 2	Assessment 3	Assessment 4	Group assignment 3	Group evaluation	Final exam	Class No	groupname	Student category	Total.marks	Grade
0	54002768	17.21	24.19	0.0	0.00	15.00	18.92	26.57	0.00	0.00	0.00	50.0	2.00	69	9	cl9tg2	domestic	57.0576	P
1	54005658	66.50	0.00	22.0	18.14	29.00	19.91	22.35	17.04	60.54	24.07	42.5	1.88	63	2	cl2tg8	domestic	70.2472	Cr
2	54010438	0.00	0.00	0.0	0.00	4.13	0.00	18.86	0.00	43.75	0.00	22.5	0.00	15	3	cl3tg7	international	17.0968	F
3	54011658	60.67	29.07	27.5	18.40	24.50	18.52	25.00	19.29	66.11	27.61	35.0	1.72	75	20	cl20tg2	domestic	80.3012	D
4	54012789	37.33	29.01	11.0	20.00	19.50	19.10	27.75	17.82	55.61	19.21	42.5	2.00	33	19	cl19tg6	domestic	49.9216	F
In [15]:
##f.
from pandas.core import datetools
import statsmodels.api as sm 

A1 = dataset["Assessment 1"] 
A1 = sm.add_constant(X)
Final = dataset["Final exam"]

lm = sm.OLS(Final, A1).fit()


##Let the variable 'Assessment 1' be the predictor and the variable 'Final exam' be the response.
##Use sm.OLS to perform a simple linear regression.
In [16]:
lm.summary()

## Get a summary of the linear model.
Out[16]:
Dep. Variable:	Final exam	R-squared:	0.097
Model:	OLS	Adj. R-squared:	0.095
Method:	Least Squares	F-statistic:	61.55
Date:	Thu, 17 May 2018	Prob (F-statistic):	2.11e-14
Time:	21:18:05	Log-Likelihood:	-2498.6
No. Observations:	578	AIC:	5001.
Df Residuals:	576	BIC:	5010.
Df Model:	1		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	37.0586	1.824	20.320	0.000	33.477	40.641
Assessment 1	0.2769	0.035	7.845	0.000	0.208	0.346
Omnibus:	2.182	Durbin-Watson:	1.887
Prob(Omnibus):	0.336	Jarque-Bera (JB):	2.245
Skew:	0.146	Prob(JB):	0.326
Kurtosis:	2.908	Cond. No.	124.
In [20]:
## g.
vs = dataset[['Assessment 1', 'Assessment 3', 'Online quiz 1 ', 'Group assignment 2']]
vs = sm.add_constant(vs)
mm = sm.OLS(Final, vs).fit()

## This time, let four variables,'Assessment 1', 'Assessment 3', 'Online quiz 1 ' and 'Group assignment 2' 
## be predictors and still set the variable 'Final exam' as response and use sm.OLS to run a multiple regression.
In [21]:
mm.summary()
##Print out the summary of this model.
Out[21]:
Dep. Variable:	Final exam	R-squared:	0.169
Model:	OLS	Adj. R-squared:	0.163
Method:	Least Squares	F-statistic:	29.05
Date:	Thu, 17 May 2018	Prob (F-statistic):	5.21e-22
Time:	21:23:29	Log-Likelihood:	-2474.6
No. Observations:	578	AIC:	4959.
Df Residuals:	573	BIC:	4981.
Df Model:	4		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	25.8300	2.377	10.866	0.000	21.161	30.499
Assessment 1	0.1571	0.039	4.042	0.000	0.081	0.233
Assessment 3	0.2030	0.048	4.222	0.000	0.109	0.297
Online quiz 1	0.1465	0.102	1.439	0.151	-0.053	0.346
Group assignment 2	0.3466	0.103	3.377	0.001	0.145	0.548
Omnibus:	4.648	Durbin-Watson:	1.910
Prob(Omnibus):	0.098	Jarque-Bera (JB):	4.716
Skew:	0.218	Prob(JB):	0.0946
Kurtosis:	2.922	Cond. No.	242.
In [23]:
##h.
Status=[]

for f in dataset['Total.marks']:
    if f < 50:
        Status.append('incomplete')
    else:
        Status.append('complete')

dataset['Status']=Status
        
dataset['Status'].head()

##In this part, use the same method applied in part e, i.e.,if the value of 'Total.marks' is smaller than 60 
##(not equal), the variable 'Status' will show 'incomplete' and 'complete' otherwise.
Out[23]:
0      complete
1      complete
2    incomplete
3      complete
4    incomplete
Name: Status, dtype: object
In [24]:
##h.continue

from sklearn.linear_model import LogisticRegression
dummy=pd.get_dummies(dataset['Status'])['complete']
y_train=dummy[0:100]
x_train=dataset['Assessment 3'][0:100]
x_train = sm.add_constant (x_train)


##Use dummy to convert the 'Status' to the numerical type and thus it could be modeled.
##Use pd.get_dummies to give dummy variables for variable 'Status-complete', i.e., if 'Status' is complete, give a
##value of 1 as the dummy variable.
##Use the dummy variable as response and use the variable 'Assessment 3' as the predictors.
##Use the first 100 data as training data to run the logistic regression.
In [26]:
##h.continue
##all parameters not specified are set to their defaults
logisticRegr = LogisticRegression()
logfit=logisticRegr.fit(x_train, y_train)
In [27]:
##h.continue
y_test=dummy[100:,]
x_test=dataset['Assessment 3'][100:,]
x_test = sm.add_constant (x_test)
pred = logisticRegr.predict(x_test)
pred
Out[27]:
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)
In [28]:
##h.continue
##Use score method to get accuracy of model
score = logisticRegr.score(x_test, y_test)
print(score)
0.7824267782426778
In [29]:
x_test=dataset['Assessment 3'][100:,]
x_test = sm.add_constant (x_test)

from sklearn import metrics
cm = metrics.confusion_matrix(y_test, pred)
print(cm)
[[ 35  90]
 [ 14 339]]
In [ ]:
